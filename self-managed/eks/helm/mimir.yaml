# Do not use this configuration in production.
# It is for demonstration purposes only.

mimir:
  # -- Base config file for Grafana Mimir and Grafana Enterprise Metrics. Contains Helm templates that are evaulated at install/upgrade.
  # To modify the resulting configuration, either copy and alter 'mimir.config' as a whole or use the 'mimir.structuredConfig' to add and modify certain YAML elements.
  config: |
    usage_stats:
      installation_mode: helm

    activity_tracker:
      filepath: /active-query-tracker/activity.log

    {{- if .Values.minio.enabled }}
    alertmanager_storage:
      backend: s3
      s3:
        access_key_id: {{ .Values.minio.rootUser }}
        bucket_name: {{ include "mimir.minioBucketPrefix" . }}-ruler
        endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000
        insecure: true
        secret_access_key: {{ .Values.minio.rootPassword }}
    {{- end }}

    # This configures how the store-gateway synchronizes blocks stored in the bucket. It uses Minio by default for getting started (configured via flags) but this should be changed for production deployments.
    blocks_storage:
      backend: s3
      bucket_store:
        {{- if index .Values "chunks-cache" "enabled" }}
        chunks_cache:
          backend: memcached
          memcached:
            addresses: {{ include "mimir.chunksCacheAddress" . }}
            max_item_size: {{ mul (index .Values "chunks-cache").maxItemMemory 1024 1024 }}
            timeout: 450ms
            max_idle_connections: 150
        {{- end }}
        {{- if index .Values "index-cache" "enabled" }}
        index_cache:
          backend: memcached
          memcached:
            addresses: {{ include "mimir.indexCacheAddress" . }}
            max_item_size: {{ mul (index .Values "index-cache").maxItemMemory 1024 1024 }}
            max_idle_connections: 150
        {{- end }}
        {{- if index .Values "metadata-cache" "enabled" }}
        metadata_cache:
          backend: memcached
          memcached:
            addresses: {{ include "mimir.metadataCacheAddress" . }}
            max_item_size: {{ mul (index .Values "metadata-cache").maxItemMemory 1024 1024 }}
            max_idle_connections: 150
        {{- end }}
        sync_dir: /data/tsdb-sync
      {{- if .Values.minio.enabled }}
      s3:
        access_key_id: {{ .Values.minio.rootUser }}
        bucket_name: {{ include "mimir.minioBucketPrefix" . }}-tsdb
        endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000
        insecure: true
        secret_access_key: {{ .Values.minio.rootPassword }}
      {{- end }}
      tsdb:
        dir: /data/tsdb
        head_compaction_interval: 15m
        wal_replay_concurrency: 3

    compactor:
      compaction_interval: 30m
      deletion_delay: 2h
      max_closing_blocks_concurrency: 2
      max_opening_blocks_concurrency: 4
      symbols_flushers_concurrency: 4
      first_level_compaction_wait_period: 25m
      data_dir: "/data"
      sharding_ring:
        wait_stability_min_duration: 1m
        
    frontend:
      parallelize_shardable_queries: true
      {{- if index .Values "results-cache" "enabled" }}
      results_cache:
        backend: memcached
        memcached:
          timeout: 500ms
          addresses: {{ include "mimir.resultsCacheAddress" . }}
          max_item_size: {{ mul (index .Values "results-cache").maxItemMemory 1024 1024 }}
      cache_results: true
      query_sharding_target_series_per_shard: 2500
      {{- end }}
      {{- if .Values.query_scheduler.enabled }}
      scheduler_address: {{ template "mimir.fullname" . }}-query-scheduler-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
      {{- end }}

    frontend_worker:
      grpc_client_config:
        max_send_msg_size: 419430400 # 400MiB
      {{- if .Values.query_scheduler.enabled }}
      scheduler_address: {{ template "mimir.fullname" . }}-query-scheduler-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
      {{- else }}
      frontend_address: {{ template "mimir.fullname" . }}-query-frontend-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
      {{- end }}

    ingester:
      ring:
        replication_factor: 1
        final_sleep: 0s
        num_tokens: 512
        tokens_file_path: /data/tokens
        unregister_on_shutdown: false
        {{- if .Values.ingester.zoneAwareReplication.enabled }}
        zone_awareness_enabled: true
        {{- end }}

    ingester_client:
      grpc_client_config:
        max_recv_msg_size: 104857600
        max_send_msg_size: 104857600

    limits:
      # Limit queries to 500 days. You can override this on a per-tenant basis.
      max_total_query_length: 12000h
      # Adjust max query parallelism to 16x sharding, without sharding we can run 15d queries fully in parallel.
      # With sharding we can further shard each day another 16 times. 15 days * 16 shards = 240 subqueries.
      max_query_parallelism: 240
      # Avoid caching results newer than 10m because some samples can be delayed
      # This presents caching incomplete results
      max_cache_freshness: 10m

    memberlist:
      abort_if_cluster_join_fails: false
      compression_enabled: false
      gossip_nodes: 3
      cluster_label: "mimir"
      cluster_label_verification_disabled: true
      join_members:
      - mimir-gossip-ring.grafana.svc.cluster.local:7946
      #- dns+{{ include "mimir.fullname" . }}-gossip-ring.grafana.svc.cluster.local:{{ include "mimir.memberlistBindPort" . }}

    querier:
      # With query sharding we run more but smaller queries. We must strike a balance
      # which allows us to process more sharded queries in parallel when requested, but not overload
      # queriers during non-sharded queries.
      max_concurrent: 16

    query_scheduler:
      # Increase from default of 100 to account for queries created by query sharding
      max_outstanding_requests_per_tenant: 800

    ruler:
      alertmanager_url: dnssrvnoa+http://_http-metrics._tcp.{{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}/alertmanager
      enable_api: true
      rule_path: /data

    {{- if or (.Values.minio.enabled) (index .Values "metadata-cache" "enabled") }}
    ruler_storage:
      {{- if .Values.minio.enabled }}
      backend: s3
      s3:
        endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000
        bucket_name: {{ include "mimir.minioBucketPrefix" . }}-ruler
        access_key_id: {{ .Values.minio.rootUser }}
        secret_access_key: {{ .Values.minio.rootPassword }}
        insecure: true
      {{- end }}
      {{- if index .Values "metadata-cache" "enabled" }}
      cache:
        backend: memcached
        memcached:
          addresses: {{ include "mimir.metadataCacheAddress" . }}
          max_item_size: {{ mul (index .Values "metadata-cache").maxItemMemory 1024 1024 }}
      {{- end }}
    {{- end }}

    runtime_config:
      file: /var/{{ include "mimir.name" . }}/runtime.yaml

    server:
      grpc_server_max_concurrent_streams: 1000
      grpc_server_max_connection_age: 2m
      grpc_server_max_connection_age_grace: 5m
      grpc_server_max_connection_idle: 1m

    store_gateway:
      sharding_ring:
        wait_stability_min_duration: 1m
        replication_factor: 1
        tokens_file_path: /data/tokens
        unregister_on_shutdown: false

  # -- Additional structured values on top of the text based 'mimir.config'. Applied after the text based config is evaluated for templates. Enables adding and modifying YAML elements in the evaulated 'mimir.config'.
  structuredConfig: {}

# RBAC configuration
rbac:
  create: true
  # -- If true, PodSecurityPolicy will be rendered by the chart on Kubernetes 1.24.
  # By default the PodSecurityPolicy is not rendered on version 1.24.
  forcePSPOnKubernetes124: false
  # -- For GKE/EKS/AKS use 'type: psp'. For OpenShift use 'type: scc'
  type: psp
  # -- podSecurityContext is the default pod security context for Mimir, GEM, gateway, and cache components.
  # When installing on OpenShift, override podSecurityContext settings with
  #
  # rbac:
  #   podSecurityContext:
  #     fsGroup: null
  #     runAsGroup: null
  #     runAsUser: null
  podSecurityContext:
    fsGroup: 10001
    runAsGroup: 10001
    runAsNonRoot: true
    runAsUser: 10001
    seccompProfile:
      type: RuntimeDefault

# (optional) Alertmanager deduplicates and groups alert notifications, 
# then routes them to a notification channel, such as email, PagerDuty, or OpsGenie.
alertmanager:
  enabled: false

minio:
  enabled: true
  mode: standalone
  rootUser: grafana-mimir
  buckets:
    - name: mimir-tsdb
      policy: none
      purge: false
    - name: mimir-ruler
      policy: none
      purge: false
    - name: enterprise-metrics-tsdb
      policy: none
      purge: false
    - name: enterprise-metrics-admin
      policy: none
      purge: false
    - name: enterprise-metrics-ruler
      policy: none
      purge: false
  persistence:
    size: 5Gi
  rootPassword: supersecret
  # Changed the mc config path to '/tmp' from '/etc' as '/etc' is only writable by root and OpenShift will not permit this.
  configPathmc: "/tmp/minio/mc/"

compactor:
  replicas: 1
  #persistentVolume:
    #size: 5Gi

distributor:
  replicas: 1

ingester:
  replicas: 1
  zoneAwareReplication:
    enabled: false
  #persistentVolume:
    #size: 5Gi

admin-cache:
  enabled: false

chunks-cache:
  enabled: false

index-cache:
  enabled: false

metadata-cache:
  enabled: false

results-cache:
  enabled: false

minio:
  enabled: true

overrides_exporter:
  replicas: 1

querier:
  replicas: 1

query_scheduler:
  replicas: 1

query_frontend:
  replicas: 1

ruler:
  replicas: 1

store_gateway:
  replicas: 1
  zoneAwareReplication:
    enabled: false
  #persistentVolume:
    #size: 5Gi

# -- A reverse proxy deployment that is meant to receive traffic for Mimir.
gateway:
  # -- The gateway is deployed by default for enterprise installations (enterprise.enabled=true).
  # Toggle this to have it deployed for non-enterprise installations too.
  enabledNonEnterprise: true
  # -- Number of replicas for the Deployment
  replicas: 1